#getting the tweets from the excel file and putting them into the variable tweets 
and creates a corpus called docs

```{r}
library(tm)
library(readxl)
library(NLP)



worksheet <- read_excel("~/2yeardatafullwithtablesclassandemition.xlsx")
View(worksheet)


docs <- Corpus(VectorSource(worksheet$text_filtered))


```


#This starts with reading a line of the corpus to check it.  Then the cleaning proccess will start
```{r}

#writeLines(as.character(docs[[2]]))
#writeLines(as.character(tweetdoc[[1]]))

docs <-tm_map(docs,content_transformer(tolower))
#tweetdoc <- tm_map(tweetdoc,content_transformer(tolower))
writeLines(as.character(docs[[21]]))
writeLines(as.character(docs[[10]]))






```




#Saves the hastags in the text 

```{r}
removeMostPunctuation<-
function (x, preserve_intra_word_dashes = FALSE) 
{
    rmpunct <- function(x) {
        x <- gsub("#", "\002", x)
        x <- gsub("[[:punct:]]+", "", x)
        gsub("\002", "#", x, fixed = TRUE)
    }
    if (preserve_intra_word_dashes) { 
        x <- gsub("(\\w)-(\\w)", "\\1\001\\2", x)
        x <- rmpunct(x)
        gsub("\001", "-", x, fixed = TRUE)
    } else {
        rmpunct(x)
    }
}
```




#Now punctuation is removed

```{r}
library(tm)
library(SnowballC)

#remove punctuation
docs <-  tm_map(docs, content_transformer(removeMostPunctuation),
    preserve_intra_word_dashes = TRUE)
#Strip digits
docs <- tm_map(docs, removeNumbers)
#remove stopwords
docs <- tm_map(docs, removeWords, c(stopwords("english"),"cancer","hutch","fredhutch","fred","via","research","can","new","get","thank","like","hutchinsonctr","come","will"))
#remove whitespace
docs <- tm_map(docs, stripWhitespace)
#Good practice to check every now and then
writeLines(as.character(docs[[10]]))
#Stem document
docs <- tm_map(docs,stemDocument)

```


#Creating TDM for getting the frequency of each word & Creating DTM for doing LDA for the document & Also gets rid of empty cells in the spreadsheets
```{r}
library(WriteXLS)



tdm <- TermDocumentMatrix(docs)

#gets rid of empty cells 
ui = unique(tdm$i)
tdm.new = tdm[ui,]

any(tdm.new==0)

allmisscols <- apply(tdm.new,2, function(x)all(is.na(x)));  
colswithallmiss <-names(allmisscols[allmisscols>0]);    
print("the columns with all values missing");    
print(colswithallmiss);




#lets us see the frequency of the words 
m <- as.matrix(tdm.new)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq=v)
rownames(d) <- NULL
head(d,10)



dtm <- DocumentTermMatrix(docs)


#gets rid of empty cells 
ui = unique(dtm$i)
dtm.new = dtm[ui,]

any(dtm.new==0)

allmisscols <- apply(dtm.new,2, function(x)all(is.na(x)));  
colswithallmiss <-names(allmisscols[allmisscols>0]);    
print("the columns with all values missing");    
print(colswithallmiss);

write.csv(d, "word frequency.csv")











```












#Creating the document term matix might be able to delete this 
```{r}
dtm <- DocumentTermMatrix(docs)



#str(dtm)
#tail.matrix(dtm)
#gets rid of any empty cells
ui = unique(dtm$i)
dtm.new = dtm[ui,]

any(dtm.new==0)

allmisscols <- apply(dtm.new,2, function(x)all(is.na(x)));  
colswithallmiss <-names(allmisscols[allmisscols>0]);    
print("the columns with all values missing");    
print(colswithallmiss);
#"scienc" %in% dtm.new
```









#Way to find how many topics we need
```{r}
library(topicmodels)
#
# get some of the example data that's bundled with the package
#
data("AssociatedPress", package = "topicmodels")

harmonicMean <- function(logLikelihoods, precision=2000L) {
  library("Rmpfr")
  llMed <- median(logLikelihoods)
  as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,
                                       prec = precision) + llMed))))
}

# The log-likelihood values are then determined by first fitting the model using for example
k = 20
burnin = 1000
iter = 1000
keep = 50

fitted <- LDA(dtm.new, k = k, method = "Gibbs",control = list(burnin = burnin, iter = iter, keep = keep) )

# where keep indicates that every keep iteration the log-likelihood is evaluated and stored. This returns all log-likelihood values including burnin, i.e., these need to be omitted before calculating the harmonic mean:

logLiks <- fitted@logLiks[-c(1:(burnin/keep))]

# assuming that burnin is a multiple of keep and

 harmonicMean(logLiks)


# generate numerous topic models with different numbers of topics
sequ <- seq(2, 50, 1) # in this case a sequence of numbers from 1 to 50, by ones.
fitted_many <- lapply(sequ, function(k) LDA(dtm.new, k = k, method = "Gibbs",control = list(burnin = burnin, iter = iter, keep = keep) ))

# extract logliks from each topic
logLiks_many <- lapply(fitted_many, function(L)  L@logLiks[-c(1:(burnin/keep))])

# compute harmonic means
hm_many <- sapply(logLiks_many, function(h) harmonicMean(h))

# inspect
plot(sequ, hm_many, type = "l")

# compute optimum number of topics
sequ[which.max(hm_many)]
## 6
```


#getting the terms 
```{r}
library(topicmodels)
m=19
tweet_lda <- LDA(dtm.new, control = list(seed=3), m)


quilt = terms(tweet_lda,6)
quilt

"science" %in% quilt
"scienc" %in% tweet_lda

```




#preproccessing to put the columns into worksheet  (Don't need this anymore)
```{r}
library(topicmodels)
m=19
lda <- LDA(dtm.new, control = list(alpha = 0.1), m)
gammaDF <- as.data.frame(lda@gamma) 
names(gammaDF) <- c(1:m)

#gammaDF

toptopics <- as.data.frame(cbind(document = row.names(gammaDF), 
  topic = apply(gammaDF,1,function(x) names(gammaDF)[which(x==max(x))])))
# inspect...
toptopics   

View(toptopics)

input1 = c(6826,5)
input2= c(6827,3)
input3= c(6828,19)
input4= c(6829,8)


toptopics=rbind(toptopics,input1,input2,input3,input4)


worksheet$newcolumn <- toptopics

```


#writing the columns into the spread sheet

```{r}


library(WriteXLS)

my.df <- data.frame(lapply(toptopics, as.character), stringsAsFactors=FALSE)

write.csv(my.df, "test3.csv")


```





#probabilty of each word for each topic ID 
```{r}
library(tidytext)
library(topicmodels)
tweet_lda

tweettop<- tidy(tweet_lda, matrix = "beta")

tweettop

```




#Highest tf-idf words in each Topic graph 

```{r}

library(ggplot2)
library(dplyr)

png(file= "Charts&stuff.png",width=2000,height=700)
tw_top_terms <- tweettop %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

tw_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = TRUE) +
  labs(x = NULL, y = "tf-idf")+
  facet_wrap(~ topic, scales = "free") +
  coord_flip()


dev.off()


```



#writes the table for the topic IDs 
```{r}


quilt2 = terms(tweet_lda,6)
quilt2



write.csv(quilt2, "test4.csv")

```



#WORD CLOUD
```{r}
library(wordcloud)
par(bg="grey30")
png(file= "wordcloudHutch.png",width=2000,height=800, bg="grey30")
wordcloud(d$word, d$freq, col=terrain.colors(length(d$word), alpha=0.9), random.order=FALSE, rot.per=0.3 )
title(main = "Most used words in Hutch's tweets",font.main = 1, col.main = "white", cex.main = 1.5)
dev.off()


```


#Gets the setment of the tweets, creates a dataframe classifing each word and creates a graph off this
```{r}
library('syuzhet')


tweets = data.frame(worksheet)
tmp<-get_nrc_sentiment(tweets$text_filtered)

tempd<-data.frame(t(tmp))#makes it a table
tmp_new <- data.frame(rowSums(tempd[1:6829]))

names(tmp_new)[1] <- "count"
tmp_new <- cbind("sentiment" = rownames(tmp_new), tmp_new)
rownames(tmp_new) <- NULL
tmp_new2<-tmp_new[1:8,]
tmp_new3<-tmp_new[9:10,]


tmp_new4<-data.frame((tmp_new3))


WordEmotion <- data.frame(x=1:nrow(tmp))


colnames(WordEmotion)<-"If the tweet was good or bad"

for (i in 1:nrow(tmp)){
  if(tmp[i,9] > tmp[i,10]){
    WordEmotion[i,1]<- "Negative"
  }else if (tmp[i,9] == tmp[i,10]) {
    WordEmotion[i,1]<- "Neutral"
  } else {
    WordEmotion[i,1]<- "Positive"
  }
}
View(WordEmotion)






write.csv(WordEmotion, "Word Emotion table.csv")



#Visualisation
library("ggplot2")
png(file="bargraphhutchemition.png",width=1000,height=700)
qplot(sentiment, data=tmp_new2, weight=count, geom="bar",fill=sentiment)+ggtitle("word sentiment count")
dev.off()


png(file="bargraphpositiveornegative.png",width=700,height=700)
qplot(sentiment, data=tmp_new3, weight=count, geom="bar",fill=sentiment)+ggtitle("word sentiments pos or neg")
dev.off()





```



#Scratch work right here 

```{r}

terms =terms(lda)
topics = topics(lda)


 r <- cbind(as.vector(X3monthsnoURL$text_no_url), as.vector(terms[topics])) 

 
#write.table(r, 'clipboard', sep='\t')

#write.table(quilt, 'clipboard', sep='\t')
write.table(topics, 'clipboard', sep='\t')


```


#who knows why this is here but don't dare delete this unless everything else is perfect
```{r}
#terms(lda,10)
topics(lda)
```





